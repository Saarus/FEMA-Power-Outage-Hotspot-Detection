{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection Trial and Error\n",
    "_**Author**: [Boom Devahastin Na Ayudhya](https://linkedin.com/in/boom-devahastin)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [TwitterScraper: Webscraping Tweets](#TwitterScraper:-Webscraping-Tweets)\n",
    "2. [Geo Data for Cities](#Geo-Data-for-Cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwitterScraper: Webscraping Tweets\n",
    "This is a package we can use to scrape historical tweets. While the ideal tool would use live Twitter data with the proper funding from clients, we would like to emphasize that this is a proof-of-concept model. Using historical tweets, we can identify posts that can be confirmed to be about power outages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitterscraper in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (0.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from twitterscraper) (4.3.0)\n",
      "Requirement already satisfied: coala-utils~=0.5.0 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from twitterscraper) (0.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from twitterscraper) (2.21.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from twitterscraper) (0.0.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\adiwid\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->twitterscraper) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No metadata found in c:\\users\\adiwid\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "# Install Twitter Scraper\n",
    "!pip install twitterscraper\n",
    "import twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, timestamp]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Packages\n",
    "from twitterscraper import query_tweets\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty template tweet dataframe to later populate \n",
    "tweet_df =pd.DataFrame(columns=[\"id\",\"text\",\"timestamp\"])\n",
    "\n",
    "# Check template\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial version scrapes posts that contain \"blackout(s)\", \"power outage\", or \"outage(s)\". Our ideal version of this query would also include an AND clause with a full list of US cities in hopes that we might infer the location from the body of the tweet text. However, the API locked us out due to attempting to pull too many posts at once so we were unable to do this within the given timeframe for the project. \n",
    "\n",
    "Since this is a proof-of-concept model, we will remedy this in Section 2 (Data Cleaning) which will involve randomly assigning a city to each post using the `NE_cities` dataframe above. This is so that we will have the chance to work with geospatial data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Tweets between specified dates\n",
    "list_of_tweets = query_tweets(\"blackout OR blackouts OR outage OR outages OR power outage\",\n",
    "                              begindate = datetime.date(2018,1,1),\n",
    "                              enddate = datetime.date(2018,12,31),\n",
    "                              poolsize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features of tweets to populate dataframe:\n",
    "for row, tweet in enumerate(list_of_tweets):\n",
    "    tweet_df.loc[row,'id'] = tweet.id\n",
    "    tweet_df.loc[row,'text'] = tweet.text\n",
    "    tweet_df.loc[row,'timestamp'] = tweet.timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine\n",
    "print(tweet_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv\n",
    "tweet_df.to_csv(\"./dirty_tweets_20180101-20181231.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Data for Cities\n",
    "We read in a file that contains geographical data (including latitude and longitude) for all cities in the United States. As a proof-of-concept model, we're restricting our visualizations to just the New England region    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>New London</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Merrimack</td>\n",
       "      <td>43.4139</td>\n",
       "      <td>-71.9844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>Peterborough</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Hillsborough</td>\n",
       "      <td>42.8790</td>\n",
       "      <td>-71.9593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>Union</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>43.4913</td>\n",
       "      <td>-71.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>Walpole</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Cheshire</td>\n",
       "      <td>43.0792</td>\n",
       "      <td>-72.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>Enfield</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Grafton</td>\n",
       "      <td>43.6448</td>\n",
       "      <td>-72.1480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city state_id     state_name   county_name      lat      lng\n",
       "4681    New London       NH  New Hampshire     Merrimack  43.4139 -71.9844\n",
       "4682  Peterborough       NH  New Hampshire  Hillsborough  42.8790 -71.9593\n",
       "4683         Union       NH  New Hampshire       Carroll  43.4913 -71.0233\n",
       "4684       Walpole       NH  New Hampshire      Cheshire  43.0792 -72.4236\n",
       "4685       Enfield       NH  New Hampshire       Grafton  43.6448 -72.1480"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in .csv\n",
    "cities = pd.read_csv('./datasets/USCFinal.csv', index_col=0)\n",
    "\n",
    "# Filter out only New England cities\n",
    "NE_cities = cities[cities['state_id'].isin([\"ME\", \"VT\", \"NH\",\"MA\", \"CT\", \"RI\"])]\n",
    "\n",
    "# Extract only city, state abbreviation, state name, county name, latitude, and longitude\n",
    "NE_cities = NE_cities.loc[:,['city', 'state_id', 'state_name', 'county_name', 'lat', 'lng']]\n",
    "\n",
    "# View\n",
    "NE_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save down to records\n",
    "NE_cities.to_csv(\"./datasets/new_england_cities_geo-data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
